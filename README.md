# üß† NeuroNova Engine ‚Äî System Architecture

<img width="1642" height="742" alt="Screenshot 2025-12-03 010304" src="https://github.com/user-attachments/assets/196439cd-e5f4-4e90-b813-c039ce80c888" />


The **NeuroNova Engine** is designed as a **dual‚Äëpipeline interaction system**, combining traditional NLP‚Äëbased emotion understanding with a lightweight **spiking neural simulation**.  
This architecture enables the system to both *interpret* user input and *simulate* corresponding neural activation patterns for visualization and research.

---

## üî∑ 1. NLP Pipeline ‚Äî Emotion Understanding & Response Generation

This pipeline processes raw user text and extracts emotional and semantic features.

### **Components**
- **Semantic Analysis Module**  
  Extracts linguistic features such as sentiment, keywords, conversational cues, and context markers.

- **Emotion Classification Layer**  
  Maps semantic features into an **emotion vector**, representing intensities across multiple emotional dimensions.

- **Emotion Output Layer**  
  Converts the emotion vector into structured data used for:  
  - Heatmap generation  
  - Spiking neural simulation  
  - Conversational responses  

- **Response Generator**  
  Produces a lightweight reply based on emotional state + conversation history.

### **Outputs**
- Emotion heatmap data  
- Auto‚Äëgenerated text response  
- Updated context memory  

---

## üü£ 2. Neural Simulation Pipeline ‚Äî Spiking Neural Activity Visualization

This pipeline transforms the emotion vector into **biologically inspired spiking activity**.

### **Components**
- **Spike Encoder**  
  Translates emotion intensity values into spike trains (temporal encoding).

- **LIF Spiking Neural Network Layer**  
  A small Leaky‚ÄëIntegrate‚Äëand‚ÄëFire (LIF) neuron population that simulates firing behavior based on encoded spikes.

- **Neural Visualization Module**  
  Generates:  
  - Firing rate graphs  
  - Membrane potential traces  
  - Intermediate activation maps

### **Outputs**
- Neuron firing rates  
- Membrane potential plots  
- Activation data for debugging

---

## üìö 3. Context History Module ‚Äî Multi‚ÄëTurn Memory

A lightweight memory module that:
- Stores emotion trends  
- Tracks previous responses  
- Updates global context  
- Improves continuity across conversation turns  

This helps the system behave consistently during longer interactions.

---

## üß© 4. System Outputs ‚Äî Final Interactive and Visual Results

| Output Type | Description |
|-------------|-------------|
| **Emotion Heatmap** | Visualizes emotional intensities across categories. |
| **Neural Firing Rates** | Spiking activity aggregated over time. |
| **Membrane Potential Graphs** | Voltage dynamics of LIF neurons. |
| **Conversation Response** | Text reply generated by the NLP pipeline. |

---

## üéØ Purpose of This Architecture

The system is designed to:
- Demonstrate emotion‚Äëaware interaction concepts  
- Explore hybrid symbolic‚Äìspiking architectures  
- Provide interpretable visualization for research and learning  
- Serve as a base for future cognitive or interaction‚Äësystem studies  

‚ö†Ô∏è **Note:**  
This is a research prototype ‚Äî *not* a medical, diagnostic, or psychological assessment tool.

---









<img width="1920" height="1020" alt="Screenshot 2025-12-02 224026" src="https://github.com/user-attachments/assets/7702e9bb-4cf1-4f79-ba1b-d2e1b12a34e3" /># NeuroNova ‚Äî conversational emotion visualization (prototype)



**Author:** Mahesh  
**Project:** NeuroNova semantic visual demo  
**Purpose:** Small research/demo tool that converts text inputs into an emotion activation heatmap, lightweight spiking-LIF simulation, and simple response templates. Intended for academic demonstration only.

> ‚ö†Ô∏è **Important safety note:** This project contains a non-clinical heuristic model that estimates emotion and a simple mental-health risk score. It is experimental and **not** a medical or diagnostic tool. If a user is in immediate danger or exhibiting self-harm intent, follow local emergency protocols and contact professional services.

<p>This prototype was created during my independent study on human-centered AI and neural activation visualization. It helped me explore how spiking models and semantic detectors can be combined for interaction research </p>
---

## What's included

- `neuronova/engine.py` ‚Äî core semantic detector +
 LIF spiking population + response generator.
- `neuronova/visualizer.py` ‚Äî matplotlib visualization helpers (interactive and non-interactive modes).
- `neuronova/cli.py` ‚Äî command-line interface.
- `neuronova/config.py` ‚Äî configuration and environment variables.
- `examples/demo_noninteractive.py` ‚Äî runs three example inputs and saves static PNG outputs to `assets/`.
- `assets/` ‚Äî pre-generated screenshots and sample images.
- `requirements.txt` ‚Äî Python packages required.

---

## üìä Visual Output

Here are sample visualizations generated by the system:

<img width="1920" height="1020" alt="Screenshot 2025-12-02 224154" src="https://github.com/user-attachments/assets/38a6ccb9-0e52-46f9-a9b4-4d4b67078c3a" />
<img width="1920" height="1020" alt="Screenshot 2025-12-02 224026" src="https://github.com/user-attachments/assets/229e6b31-3163-43a6-9906-2899fcecb60b" />

The visualizations show:
- **Emotion activation heatmap** ‚Äî Real-time emotion detection across 27 emotions
- **Mental health risk assessment** ‚Äî Risk score tracking over conversation turns
- **Neural firing rates** ‚Äî Spiking neural network activity over time
- **Membrane potentials** ‚Äî Individual neuron voltage traces showing spiking behavior

---

## Quick start (non-interactive demo)

1. Create a virtual environment (recommended):
   ```bash
   python3 -m venv venv
   source venv/bin/activate 
   pip install -r requirements.txt
   ```

2. Run demo (writes outputs to assets/):
   ```bash
   python examples/demo_noninteractive.py
   ```

3. Inspect `assets/screenshot1.png`, `assets/screenshot2.png`, and `assets/transcript.txt`.

---

## Optional interactive mode

If you want to try interactive graphs, run:

```bash
python -m neuronova.cli
```

Then at the prompt type sentences or `graph` to open the conversation graph. (Interactive plotting requires a GUI-enabled environment.)

---

## Configuration & Security

- `GEMINI_API_KEY` or `OPENAI_API_KEY` can be set as environment variables if you want remote LLM fallback (optional).
- To disable remote API usage for privacy, do not set those environment variables.
- By default, the system will not send any user input to remote APIs unless explicitly configured.

---

## Ethical considerations

- The mental-health detection is heuristic and may produce false positives/negatives.
- Do not deploy as a clinical tool.
- The insult word list in `data/insult_words.txt` is used only for detection purposes. Remove or restrict if distributing in contexts where offensive terms are not acceptable.

---

## License

MIT ‚Äî see LICENSE file.

---

<h4>In the future, I plan to extend this system with explainable AI modules and improved temporal modeling</h4>

## Contact

If you'd like to discuss the methodology or see a live walkthrough,
you can reach me at:

Email: mahesh9880302264.v@gmail.com
LinkedIn: https://www.linkedin.com/in/mr-mahesh-4209b4284


